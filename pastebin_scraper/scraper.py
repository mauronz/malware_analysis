import re
import mechanize
import hashlib
import os.path
import requests
import time
import sqlite3
import yaml
import binascii
import urllib2
from bs4 import BeautifulSoup
from dateutil import parser

req_count = 0

def vt_add_comment(code):
	global req_count
	if req_count == 4:
		req_count = 0
		time.sleep(65)
	params = {'apikey':  VT_KEY, 'resource': code, 'comment': "Found as a base64 encoded string in #pastebin using #pastemonitor"}
	response = requests.post('https://www.virustotal.com/vtapi/v2/comments/put', params=params)	
	
	req_count += 1


def vt_send_file(filename):
	global req_count
	if req_count == 4:
		req_count = 0
		time.sleep(65)
	params = {'apikey': VT_KEY}
	files = {'file': ("from_pastebin", open("binaries/" + filename, 'rb'))}
	response = requests.post('https://www.virustotal.com/vtapi/v2/file/scan', files=files, params=params)
	json_response = response.json()

	req_count += 1

	return json_response["scan_id"]


def vt_request_report(code):
	global req_count
	if req_count == 4:
		req_count = 0
		time.sleep(65)
	params = {'apikey':  VT_KEY, 'resource': code}
	headers = { "Accept-Encoding": "gzip, deflate", "User-Agent" : "gzip,  My Python requests library example client or username" }
	response = requests.get('https://www.virustotal.com/vtapi/v2/file/report', params=params, headers=headers)	
	json_response = response.json()
	
	req_count += 1

	return json_response


def analyze_vt(hsh, conn):
	c = conn.cursor()

	is_new = False
	while True:
		json = vt_request_report(hsh)
		if json["response_code"] == 0:
			vt_send_file(hsh)
			is_new = True
		elif json["response_code"] == 1:
			print "\tDetection: %d/%d" % (json["positives"], json["total"])
			name1 = ""
			name2 = ""
			if NAMING_AV1 in json["scans"]:
				name1 = json["scans"][NAMING_AV1]["result"]
				print "\tName: %s (%s)" % (name1, NAMING_AV1)
			if NAMING_AV2 in json["scans"]:
				name2 = json["scans"][NAMING_AV2]["result"]
				print "\tName: %s (%s)" % (name2, NAMING_AV2)

			c.execute("INSERT INTO Binaries(sha256, positives, total, name1, name2, scan_url) VALUES (?, ?, ?, ?, ?, ?)", (hsh, json["positives"], json["total"], name1, name2, json["permalink"]))
			conn.commit()		

			if is_new:
				vt_add_comment(hsh)
			break
		elif json["response_code"] != -2:
			print "Unknow response code from VT: %d" % json["response_code"]
			break

		print "Report in queue"
		time.sleep(30)		


def analyze_page(br, conn):	

	hashes = []

	reg = re.compile("^/archive/a/.+$")
	c = conn.cursor()

	times = []
	soup = BeautifulSoup(br.response().read(), "html.parser")
	for tag in soup.find_all("span", class_="specific-time"):
		date = parser.parse(tag["title"])
		times.append("%d-%02d-%02d %02d:%02d:00" % (date.year, date.month, date.day, date.hour, date.minute))

	count = 0
	for link in br.links(url_regex=reg):
		url_id = link.url[11:]
		c.execute("SELECT COUNT(*) FROM Dumps WHERE id=?", (url_id,))
		res = int(c.fetchone()[0])
		if res == 0:
			try:
				resp = br.follow_link(link)
				data = resp.get_data()
				f = open("cur_bas64.txt", "w")
				f.write(data)
				f.close()
				try:
					binary = data.decode("base64")
				except binascii.Error:
					print "Error decoding link %s" % (link.url)
					f = open("invalid/" + url_id, "w")
					f.write(data)
					f.close()				
					c.execute("INSERT INTO Dumps(id, time, sha256) VALUES(?, ?, ?)", (url_id, times[count], "FALSE_POSITIVE"))
					count += 1
					conn.commit()
					br.back()
					continue			

				if binary[0:2] != "MZ":
					print "False positive %s" % (link.url)				
					f = open("invalid/" + url_id, "w")
					f.write(data)
					f.close()				
					c.execute("INSERT INTO Dumps(id, time, sha256) VALUES(?, ?, ?)", (url_id, times[count], "FALSE_POSITIVE"))	
					count += 1
					conn.commit()
					br.back()
					continue

				hsh = hashlib.sha256(binary).hexdigest()
				if not os.path.exists("binaries/" + hsh) :
					print "New file: %s" % hsh
					f = open("binaries/" + hsh, "w")
					f.write(binary)
					f.close()

					hashes.append(hsh)
	
				else:
					print "Already seen: %s" % hsh
	
				c.execute("INSERT INTO Dumps(id, time, sha256) VALUES(?, ?, ?)", (url_id, times[count], hsh))
				count += 1
				conn.commit()

				br.back()
			except urllib2.HTTPError:
				count += 1
				pass

	return hashes


if __name__ == "__main__":

	ymlfile = open("config.yml", 'r')
	cfg = yaml.load(ymlfile)
	MONITOR_USER = cfg["scraper"]["monitor_user"]
	MONITOR_PWD = cfg["scraper"]["monitor_pwd"]	
	VT_KEY = cfg["scraper"]["vt_key"]
	NAMING_AV1 = cfg["scraper"]["naming_av1"]
	NAMING_AV2 = cfg["scraper"]["naming_av2"]


	while True:
		conn = sqlite3.connect("database.sqlite")

		br = mechanize.Browser()
		br.open("https://www.pastemonitor.com/account/login")
		form = br.forms()[0]
		br.form = form
		form["username"] = MONITOR_USER
		form["password"] = MONITOR_PWD
		resp = br.submit()
		if resp.geturl() == "https://www.pastemonitor.com/account/login":
			print "Error: Wrong credentials for pastemonitor"
			exit()

		#hashes = []

		cur_page = 1
		while True:
			#hashes += analyze_page(br, conn)
			analyze_page(br, conn)
			try:
				link = br.find_link(url="/portal/i?p=%d" % (cur_page+1))
				br.follow_link(link)
				cur_page += 1
			except mechanize.LinkNotFoundError:
				break

		#print "Pastemonitor scan completed: %d new samples" % len(hashes)
		print "Pastemonitor scan completed"		
		print
		print "Starting the analysis of new samples..."

		c = conn.cursor()
		c.execute("SELECT DISTINCT sha256 FROM Dumps WHERE sha256<>\"FALSE_POSITIVE\" AND sha256 NOT IN (SELECT sha256 FROM Binaries)")

		tmp = c.fetchall()
		hashes = []
		for row in tmp:
			hashes.append(row[0])

		for i, hsh in enumerate(hashes):
			print "Sample %d of %d" % (i+1, len(hashes))
			analyze_vt(hsh, conn)
			print

		conn.close()
		break
		
	
		
	
